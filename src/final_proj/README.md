## Abstract
I created a PyGame project that incorporated multiple AI techniques. The game puts cities and routes on a map with various elevations and spawns in an elf sprite which travels between cities. The elf will randomly encounter combat whilst travelling and fight using a sword, fire, or bow. The elf will also lose money for each route he travels because they are associated with a cost. The goal of the elf, named Legolas, is to get to city 9, then he will win. Legolas will lose in the situations in which he runs out of money or dies in combat. Several AI techniques were incorporated into the project such that the game could be both entertaining and functional. These AI techniques included a Genetic Algorithm with a fitness function, in which the final cities used for the game were developed through many iterations of cities that were both in plausible elevations and appropriate distances from each other. Another AI technique used was Reinforcement Learning, which was used to make Legolas our character, pretrained on how to fight before venturing the world. The final AI technique was using a pretrained masked language model, in which the game was able to give weather updates by having a sentence with a mask and picking from the generated responses. In conclusion, by combining multiple AI techniques, I created a PyGame project that not only provided entertainment but also demonstrated the application of AI in real-world problems. The game was a representation of how AI on a smaller scale can be used within larger gaming experiences and create unique scenarios/opportunities.
## AI Components Used

 1. Genetic Algorithms
 2. Reinforcement Learning
 3. Masked Language Model
## Problems Solved
1.  Realistic Cities The genetic algorithm used in this project aimed to generate realistic cities on the game map. To achieve this, the algorithm used a fitness function that evaluated each iteration of cities generated by the algorithm. The fitness function was designed to prioritize cities that were ideally elevated and evenly spaced apart.

The genetic algorithm worked by generating a population of random cities, with each city represented as a set of coordinates on the game map. Each city was evaluated by the fitness function, which returned a fitness score for the city. The genetic algorithm then selected the top-performing cities from the population and used them to create new cities through crossover and mutation operations. This process was repeated over multiple generations until a set of cities with optimal fitness scores was generated.

The inputs for this algorithm were the game map dimensions and constraints, such as minimum and maximum city distances and elevations. The outputs were a set of city coordinates that were realistic and suitable for use in the game.

2.  Win Every Fight Reinforcement learning was used in this project to train Legolas to win every fight in the game. The game will indeed end if Legolas does lose in battle and this algorithm will not account for losing by running out of budget, however the game implements Monte Carlo reinforcement learning such that Legolas becomes an all-winning fighter. Reinforcement learning is a type of machine learning where an agent learns to make decisions in an environment by maximizing a reward signal. In this case, the reward signal was the outcome of each fight in the game, where winning a fight resulted in a positive reward and losing resulted in a negative reward.


The game incorporates the use of Reinforcement Learning (RL) to make Legolas an optimal fighter. Specifically, the Monte Carlo method is used for training. Before the game begins, Legolas goes through extensive training to learn how to win every fight he encounters in the game.

In the Monte Carlo method, the agent learns by repeatedly playing the game and observing the rewards it receives for each action taken. The agent then uses this experience to update its policy, which is the strategy it uses to choose actions in the future.

During training, Legolas plays against a random computer opponent in a simulated combat scenario. For each action he takes, Legolas receives a reward based on the outcome of the fight. If he wins, he receives a positive reward; if he loses, he receives a negative reward. The goal is to maximize the cumulative reward obtained over time.

After each training episode, the Monte Carlo method updates the value function, which is a function that estimates the expected cumulative reward for each state-action pair. The value function is used to guide the agent's decision-making process by helping it choose actions that are likely to lead to higher rewards.

The Monte Carlo method has several advantages, including its simplicity and ability to handle large state spaces. However, it can be computationally expensive, as it requires the agent to play many episodes of the game to obtain accurate estimates of the value function.

In the context of the game, the Monte Carlo method allows Legolas to learn how to win every fight he encounters by repeatedly playing simulated combat scenarios and adjusting his strategy based on the rewards he receives. This not only makes the game more challenging for the player but also showcases the potential applications of RL in developing intelligent agents for various real-world scenarios.

3.  Weather Log The masked language model used in this project was a pre-trained model called BERT (Bidirectional Encoder Representations from Transformers). BERT is a state-of-the-art language model that can generate high-quality text by predicting missing words in a sentence.


BERT is a language model developed by Google, capable of generating high-quality natural language text.

The game utilizes BERT by inputting a sentence with a mask, specifically "The weather is {MASK}". The mask is a special token in the input sentence that represents a missing word. BERT then generates a list of possible candidate words that could fit in the masked position, along with a probability score for each candidate.

To generate the weather update, the game selects a candidate word from the list of possibilities generated by BERT and inserts it into the sentence in place of the mask. The output sentence is then displayed to the user as the weather update.

BERT uses a technique called self-attention, which allows the model to consider the context of each word in the sentence when generating its representation. This enables the model to capture complex relationships between words and generate high-quality natural language text.

One of the key advantages of using pre-trained language models like BERT is that they can be fine-tuned for specific tasks with relatively little additional training data. This makes them an ideal choice for tasks like generating weather updates, where the input sentence structure is predictable and the model can leverage its pre-existing knowledge of natural language.

In conclusion, the use of BERT in the game demonstrates the potential of pre-trained language models in generating natural language text for various real-world applications. By inputting a sentence with a mask and selecting a candidate word from the list of possibilities generated by BERT, the game is able to generate realistic weather updates for the player, adding an additional layer of immersion and engagement to the gaming experience.























































## Appendix
ChatGPT conversation:
I created a PyGame project that incorporated multiple AI techniques. The game puts cities and routes on a map with various elevations and spawns in an elf sprite which travels between cities. The elf will randomly encounter combat whilst travelling and fight using a sword, fire, or bow. The elf will also lose money for each route he travels because they are associated with a cost. The goal of the elf, named Legolas, is to get to city 9, then he will win. Legolas will lose in the situations in which he runs out of money or dies in combat. Several AI techniques were incorporated into the project such that the game could be both entertaining and functional. These AI techniques included a Genetic Algorithm with a fitness function, in which the final cities used for the game were developed through many iterations of cities that were both in plausible elevations and appropriate distances from each other. Another AI technique used was Reinforcement Learning, which was used to make Legolas our character, pretrained on how to fight before venturing the world. The final AI technique was using a pretrained masked language model, in which the game was able to give weather updates by having a sentence with a mask and picking from the generated responses. In conclusion, by combining multiple AI techniques, I created a PyGame project that not only provided entertainment but also demonstrated the application of AI in real-world problems. The game was a representation of how AI on a smaller scale can be used within larger gaming experiences and create unique scenarios/opportunities. It solved three problems/used three AI techniques. I will give an explanation of each of the problems and I want you to use what I give and reporduce to me a better explanation of each, give more details on the algorithms and more information in general, feel free to use up to 1500 words:  1. Realistic Cities: the cities were able to be realistically generated due to the genetic algorithm and corresponding fitness function. With aid of the fitness function, each iteration of the genetic algorithm would have cities that are more fit which in our scenario meant they were either more ideally elevated (not on mountains or in lakes) and spaced evenly apart (no closer than 20 pixels)
2. Win Every Fight: The game takes the role of a simulation at its current state, being that once the user runs it, it runs autonomously until it ends. One AI technique used was Reinforcement Learning, in which before the game starts, Legolas is put through an extensive training in order to in every fight. The training algorithms get the returns of action values to eventually create an optimal policy, whcih allows Legolas when the game begins to win every fight vs the computer.
3. Weather Log: Outside of the game having a budget, fights, and routes I have added the capability to generate a weather statement. This is done using tensorflow and a model named Bert Base Uncased in which the sentence "The weather is {MASK}" is inputted and output is a response in which gives possible AI generated unmasks that are later randomly selected to give the game a more involved feeling.
ChatGPT
Realistic Cities: The cities in the game were realistically generated through the use of a genetic algorithm and a corresponding fitness function. The aim of the fitness function was to generate cities that were not only ideally elevated but also spaced evenly apart. The genetic algorithm involved the creation of several iterations of potential cities. Each iteration was evaluated using the fitness function, which assigned a fitness score to each city. The fitness score was determined based on the elevation of the city and the distance between the cities. The higher the score, the more fit the city was deemed to be. As the iterations continued, the cities became more fit, resulting in a final set of cities that were both ideally elevated and spaced evenly apart.
The genetic algorithm used in this project involves a population of potential solutions that undergoes evolution and selection to produce the best possible solution. In each iteration, the population undergoes reproduction, mutation, and selection to create a new population that is more fit than the previous one. The fitness function used in this project assigns a fitness score to each solution in the population based on how well it satisfies the criteria. The algorithm then selects the most fit solutions for the next generation, allowing the population to evolve over time.

Win Every Fight: The game takes the role of a simulation, which means it runs autonomously until it ends. To ensure that Legolas, the elf sprite, wins every fight in the game, reinforcement learning was used. Reinforcement learning involves training an agent, in this case Legolas, to make decisions in an environment based on feedback received through rewards and punishments. Before the game starts, Legolas is put through an extensive training process to learn how to fight effectively.
During the training process, Legolas is placed in various combat scenarios and given feedback based on the outcome of each scenario. The feedback is in the form of a reward or a punishment, depending on whether Legolas wins or loses the fight. Over time, the algorithm learns to associate specific actions with rewards or punishments and eventually creates an optimal policy. The optimal policy allows Legolas to make the best possible decision in any combat scenario and emerge victorious.

Weather Log: In addition to the budget, fights, and routes, the game also includes a weather log generated using a pre-trained masked language model called Bert Base Uncased. The weather log adds an extra level of immersion to the game, allowing players to feel more involved in the game world. The weather log is generated by inputting the sentence "The weather is {MASK}" into the language model. The model then generates several possible unmasks, which are later randomly selected to give the game a more realistic weather update.
Bert Base Uncased is a pre-trained language model that can be used for a variety of natural language processing tasks, including text classification, sentiment analysis, and language translation. The model is based on a transformer architecture that can efficiently process large amounts of text data. In this project, the model was used to generate weather updates, providing a more immersive gaming experience. The use of a pre-trained language model eliminates the need for extensive training and allows for quick and accurate generation of responses.





User

2 / 2

this is a good base but it needs about 500 more words, go in depth on the inputs and outputs of the algorithms if you have not done so already
ChatGPT
Sure, I'd be happy to provide more details on each of the AI techniques used in the PyGame project and the inputs and outputs of their respective algorithms.

Realistic Cities
The genetic algorithm used in this project aimed to generate realistic cities on the game map. To achieve this, the algorithm used a fitness function that evaluated each iteration of cities generated by the algorithm. The fitness function was designed to prioritize cities that were ideally elevated and evenly spaced apart.
The genetic algorithm worked by generating a population of random cities, with each city represented as a set of coordinates on the game map. Each city was evaluated by the fitness function, which returned a fitness score for the city. The genetic algorithm then selected the top-performing cities from the population and used them to create new cities through crossover and mutation operations. This process was repeated over multiple generations until a set of cities with optimal fitness scores was generated.

The inputs for this algorithm were the game map dimensions and constraints, such as minimum and maximum city distances and elevations. The outputs were a set of city coordinates that were realistic and suitable for use in the game.

Win Every Fight
Reinforcement learning was used in this project to train Legolas to win every fight in the game. Reinforcement learning is a type of machine learning where an agent learns to make decisions in an environment by maximizing a reward signal. In this case, the reward signal was the outcome of each fight in the game, where winning a fight resulted in a positive reward and losing resulted in a negative reward.
The reinforcement learning algorithm used in this project was a variant of the Q-learning algorithm, which is a model-free, off-policy, and value-based algorithm. The Q-learning algorithm works by estimating the optimal action-value function, which gives the expected cumulative reward for taking a particular action in a particular state. The optimal policy, which gives the best action to take in each state, can then be derived from the action-value function.

To train Legolas using reinforcement learning, a simulation of the game was run many times with different initial conditions and random actions. Legolas was initialized with random weights, which were updated using the Q-learning algorithm during each iteration of the simulation. The inputs for the reinforcement learning algorithm were the current state of the game, including Legolas' health and the enemy's health, as well as the actions available to Legolas. The output was the action selected by Legolas based on the Q-values estimated by the algorithm.

Weather Log
The masked language model used in this project was a pre-trained model called BERT (Bidirectional Encoder Representations from Transformers). BERT is a state-of-the-art language model that can generate high-quality text by predicting missing words in a sentence.
To generate weather updates in the game, a sentence template "The weather is {MASK}" was inputted to the BERT model. The model then generated a set of possible AI-generated unmasked weather statements, which were later randomly selected to give the game a more involved feeling.

The inputs for this algorithm were the masked sentence template and the pre-trained BERT model. The output was a set of possible unmasked weather statements generated by the model.

Overall, the PyGame project demonstrates the application of multiple AI techniques in a gaming context to create an engaging and realistic experience. The genetic algorithm was used to generate realistic cities on the game map, while reinforcement learning was used to train Legolas to win every fight in the game. The masked language model was used to generate weather updates in the game, providing an additional layer of immersion for the player.





User
rewrite #2 to talk about monte carlo, not Q-learning
ChatGPT
The game incorporates the use of Reinforcement Learning (RL) to make Legolas an optimal fighter. Specifically, the Monte Carlo method is used for training. Before the game begins, Legolas goes through extensive training to learn how to win every fight he encounters in the game.

In the Monte Carlo method, the agent learns by repeatedly playing the game and observing the rewards it receives for each action taken. The agent then uses this experience to update its policy, which is the strategy it uses to choose actions in the future.

During training, Legolas plays against a random computer opponent in a simulated combat scenario. For each action he takes, Legolas receives a reward based on the outcome of the fight. If he wins, he receives a positive reward; if he loses, he receives a negative reward. The goal is to maximize the cumulative reward obtained over time.

After each training episode, the Monte Carlo method updates the value function, which is a function that estimates the expected cumulative reward for each state-action pair. The value function is used to guide the agent's decision-making process by helping it choose actions that are likely to lead to higher rewards.

The Monte Carlo method has several advantages, including its simplicity and ability to handle large state spaces. However, it can be computationally expensive, as it requires the agent to play many episodes of the game to obtain accurate estimates of the value function.

In the context of the game, the Monte Carlo method allows Legolas to learn how to win every fight he encounters by repeatedly playing simulated combat scenarios and adjusting his strategy based on the rewards he receives. This not only makes the game more challenging for the player but also showcases the potential applications of RL in developing intelligent agents for various real-world scenarios.





User
and also talk more about how bert uses AI to predict the words for the mask
ChatGPT
The game also incorporates the use of a pre-trained masked language model called BERT (Bidirectional Encoder Representations from Transformers) to generate weather updates for the game. BERT is a state-of-the-art language model developed by Google, capable of generating high-quality natural language text.

The game utilizes BERT by inputting a sentence with a mask, specifically "The weather is {MASK}". The mask is a special token in the input sentence that represents a missing word. BERT then generates a list of possible candidate words that could fit in the masked position, along with a probability score for each candidate.

To generate the weather update, the game selects a candidate word from the list of possibilities generated by BERT and inserts it into the sentence in place of the mask. The output sentence is then displayed to the user as the weather update.

BERT uses a technique called self-attention, which allows the model to consider the context of each word in the sentence when generating its representation. This enables the model to capture complex relationships between words and generate high-quality natural language text.

One of the key advantages of using pre-trained language models like BERT is that they can be fine-tuned for specific tasks with relatively little additional training data. This makes them an ideal choice for tasks like generating weather updates, where the input sentence structure is predictable and the model can leverage its pre-existing knowledge of natural language.

In conclusion, the use of BERT in the game demonstrates the potential of pre-trained language models in generating natural language text for various real-world applications. By inputting a sentence with a mask and selecting a candidate word from the list of possibilities generated by BERT, the game is able to generate realistic weather updates for the player, adding an additional layer of immersion and engagement to the gaming experience.